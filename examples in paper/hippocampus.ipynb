{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import sys\n",
    "sys.path.append('../code/');\n",
    "\n",
    "import encoding as enc\n",
    "import decoding as dec\n",
    "import util_plot as uplot\n",
    "\n",
    "seed = 888;\n",
    "np.random.seed(seed) # fix randomness\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deconvolution in matlab (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('../data/Hippocampus/results (12).mat');\n",
    "c = C_raw';\n",
    "shape = size(c);\n",
    "K = shape(2);\n",
    "T = shape(1);\n",
    "cd = zeros(K,T);\n",
    "s = zeros(K,T);\n",
    "bs = zeros(K,1);\n",
    "pars = zeros(K,1);\n",
    "sn = zeros(K,1);\n",
    "for n = 1:K\n",
    "    [cd(n,:), s(n,:), options] = deconvolveCa(c(:,n), 'foopsi', 'ar1', 'smin', 0, 'optimize_pars', false, 'pars', 0.95, 'optimize_b', false);\n",
    "    bs(n,1) = options.b;\n",
    "    pars(n,1) = options.pars;\n",
    "    sn(n,1) = options.sn;\n",
    "end\n",
    "save(\"../data/Hippocampus/12_s_0_deconv_new.mat\", 'cd', 's', 'bs', 'pars', 'sn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load deconvolved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load deconvolved data (do deconvolution in matlab using hard-threshold method, set smin=0)\n",
    "import h5py\n",
    "with h5py.File('../data/Hippocampus/SessInfo (15).mat', 'r') as f:\n",
    "    print(list(f.keys()))\n",
    "    print(list(f['SessInfo']))\n",
    "    print(list(f['SessInfo']['Behavior']))\n",
    "    info = (f['SessInfo']['Behavior']['treadPos'])[()];\n",
    "\n",
    "deconv = sio.loadmat(\"../data/Hippocampus/12_c_0_deconv_new.mat\");\n",
    "len_use = 53600;\n",
    "hd = info[:len_use,0].copy();\n",
    "Y_real = (deconv['s']/deconv['sn']).T[:len_use,:]; # normalize data by noise level\n",
    "hd[hd==0] = 1;\n",
    "hd = hd*360 - 180;\n",
    "\n",
    "# downsample data\n",
    "Y_real = Y_real.reshape(2, int(Y_real.shape[0]/2), Y_real.shape[1], order=\"F\").sum(axis=0);\n",
    "hd = hd.reshape(2, int(hd.shape[0]/2), order=\"F\").mean(axis=0);\n",
    "\n",
    "# only use running data\n",
    "k = 6;\n",
    "(hd[k:] - hd[:-k] == 0).sum()/hd.shape\n",
    "tempp = np.where((hd[k:] - hd[:-k] != 0))[0]\n",
    "tempq = np.where((hd[k:] - hd[:-k] == 0))[0]\n",
    "hd_enc = hd[tempp];\n",
    "Y_real_enc = Y_real[tempp,:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## result saving path\n",
    "path = './hippo_rlt/';\n",
    "\n",
    "## encoding params\n",
    "n_epochs = 1800; # number of epochs \n",
    "learning_rate = 1e-2; # step size \n",
    "gen_nodes = 15; # number of nodes used in hidden layers\n",
    "\n",
    "thresh = 0; # threshold for bernoulli model\n",
    "gam_shift = 1e-4; # shift for gamma model, in general take it as a small number\n",
    "factor = np.ones(Y_real.shape[1])*thresh; # location parameter for SNG model\n",
    "\n",
    "## decoding params\n",
    "bin_len = 121; # number of bins used for bayesian decoding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ramdomly split data\n",
    "np.random.seed(seed);\n",
    "perm = np.random.permutation(56);\n",
    "cperm = np.random.permutation(Y_real_enc.shape[1]);\n",
    "\n",
    "Y_split = np.array_split(Y_real_enc, 56, axis=0);\n",
    "hd_split = np.array_split(hd_enc, 56, axis=0);\n",
    "\n",
    "Y_train = [Y_split[index] for index in perm[:38]];\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "hd_train = [hd_split[index] for index in perm[:38]]\n",
    "hd_train = np.concatenate(hd_train, axis=0)\n",
    "\n",
    "Y_valid = [Y_split[index] for index in perm[38:44]]\n",
    "Y_valid = np.concatenate(Y_valid, axis=0)\n",
    "hd_valid = [hd_split[index] for index in perm[38:44]]\n",
    "hd_valid = np.concatenate(hd_valid, axis=0)\n",
    "\n",
    "Y_test = [Y_split[index] for index in perm[44:]]\n",
    "Y_test = np.concatenate(Y_test, axis=0)\n",
    "hd_test = [hd_split[index] for index in perm[44:]]\n",
    "hd_test = np.concatenate(hd_test, axis=0)\n",
    "\n",
    "nTrain = Y_train.shape[0];\n",
    "nValid = Y_valid.shape[0];\n",
    "nTest = Y_real_enc.shape[0]-nTrain-nValid;\n",
    "\n",
    "Y_const = np.concatenate([Y_split[index] for index in perm], axis=0);\n",
    "hd_const = np.concatenate([hd_split[index] for index in perm], axis=0);\n",
    "\n",
    "hd_bins_use = np.linspace(-180,180,bin_len)[1:];\n",
    "hd_temp = uplot.getBins(hd_const, hd_bins_use);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train encoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## poisson\n",
    "spl_values_poi,yfit_poi,cost_poi,test_cost_poi=enc.poissonRunner(Y_train,Y_valid,hd_train,hd_valid,hd_test,n_epochs=n_epochs,learning_rate=learning_rate,gen_nodes=gen_nodes,bin_len=bin_len,path=path);\n",
    "\n",
    "## bernoulli\n",
    "spl_values_ber, yfit_ber, cost_ber, test_cost_ber = enc.bernoulliRunner(Y_train,Y_valid,hd_train,hd_valid,hd_test, thresh=thresh,n_epochs=n_epochs,learning_rate=learning_rate,gen_nodes=gen_nodes,bin_len=bin_len,path=path);\n",
    "\n",
    "## gamma\n",
    "spl_values_gam,spl_k_values_gam,spl_theta_values_gam,yfit_gam,gam_k,gam_theta,cost_gam,test_cost_gam=enc.gammaRunner(Y_train,Y_valid,hd_train,hd_valid,hd_test,factor=gam_shift,n_epochs=n_epochs,learning_rate=learning_rate,gen_nodes=gen_nodes,bin_len=bin_len,path=path);\n",
    "\n",
    "## sng\n",
    "spl_values_sngr, spl_p_values_sngr, spl_theta_values_sngr, sngr_k, sngr_loc, yfit_sngr, sngr_p, sngr_theta, cost_sngr, test_cost_sngr = enc.sngRlxRunner(Y_train, Y_valid, hd_train,hd_valid, hd_test, factor, n_epochs=n_epochs,learning_rate=5e-3, gen_nodes=gen_nodes,bin_len=bin_len,path=path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian decoding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## poisson\n",
    "poi_mean_decode, poi_decode, poi_lik_mat = dec.poissonDecoding(Y_const, spl_values_poi, bin_len=bin_len);\n",
    "\n",
    "## bernoulli\n",
    "ber_mean_decode, ber_decode, ber_lik_mat = dec.bernoulliDecoding(Y_const, thresh, spl_values_ber, bin_len=bin_len);\n",
    "\n",
    "## gamma\n",
    "gam_mean_decode,gam_decode,gam_lik_mat=dec.gammaDecoding(Y_const,spl_k_values_gam,spl_theta_values_gam,factor=gam_shift,bin_len=bin_len);\n",
    "\n",
    "## SNG\n",
    "sngr_mean_decode, sngr_decode, sngr_lik_mat = dec.sngRlxDecoding(Y_const, spl_theta_values_sngr, \n",
    "                                                           spl_p_values_sngr, sngr_k, sngr_loc, bin_len=bin_len);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check encoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample data\n",
    "y_poisson, p_poi = enc.samplePoisson(yfit_poi, seed=seed);\n",
    "y_bernoulli = enc.sampleBernoulli(yfit_ber, seed=seed);\n",
    "y_sngr = enc.sampleSngRlx(sngr_p, sngr_k, sngr_theta, sngr_loc, seed=seed);\n",
    "y_gamma = enc.sampleGamma(gam_k, gam_theta, seed=seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot fitted vs observed mean\n",
    "fig_rate = uplot.plotSampleRatem(Y_const, hd_const, y_poisson,\n",
    "                                 y_bernoulli, y_gamma, y_sngr,ss=0,bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot fitted vs observed variance\n",
    "fig_var = uplot.plotSampleVarm(Y_const,y_poisson, y_bernoulli, y_gamma,\n",
    "                               y_sngr,hd_const,ss=0,bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot fitted vs empirical cdf\n",
    "nid = 10;\n",
    "fig_cdfslab = uplot.plotCdfSlabm(Y_const, hd_temp, spl_values_poi,spl_values_ber,\n",
    "                   spl_theta_values_sngr, sngr_k, sngr_loc,spl_p_values_sngr,\n",
    "                   spl_theta_values_gam, spl_k_values_gam,hd_bins_use,\n",
    "           bin_list=[0,20,40], size=[1,3], nid=nid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot fitted tunning curve\n",
    "num=1; \n",
    "nid = 10;\n",
    "tuning_curve = enc.get_tc(Y_train, hd_train, bin_len);\n",
    "tuning_curve2 = enc.get_tc(Y_test, hd_test,bin_len);\n",
    "fig_tc = uplot.plotTC(num, spl_values_poi[:,nid:], \n",
    "                      spl_values_ber[:,nid:], spl_values_gam[:,nid:], spl_values_sngr[:,nid:],\n",
    "             tuning_curve[:,nid:],tuning_curve2[:,nid:],bin_len);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot fitted params\n",
    "fig_params=uplot.plotParams([spl_values_poi, spl_theta_values_sngr, spl_p_values_sngr],\n",
    "                 ['Mean $\\mathbf{\\lambda}$','Scale $\\mathbf{a}$', 'Non-zero prob $\\mathbf{p}$'], bin_len);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check decoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute decoding mean absolute error\n",
    "\n",
    "_,poi_error = dec.error(poi_mean_decode[:], hd_const[:]);\n",
    "_,ber_error = dec.error(ber_mean_decode[:], hd_const[:]);\n",
    "_,sngr_error = dec.error(sngr_mean_decode[:], hd_const[:]);\n",
    "_,gam_error = dec.error(gam_mean_decode[:], hd_const[:]);\n",
    "\n",
    "# training error\n",
    "print('training error: poisson:', np.round(np.abs(poi_error[:nTrain]).sum()/nTrain,2),\n",
    "      'bernoulli:', np.round(np.abs(ber_error[:nTrain]).sum()/nTrain,2),\n",
    "      'gamma:', np.round(np.abs(gam_error[:nTrain]).sum()/nTrain,2),\n",
    "      'SNG:', np.round(np.abs(sngr_error[:nTrain]).sum()/nTrain,2))\n",
    "\n",
    "# test error\n",
    "print('test error: poisson:', np.round(np.abs(poi_error[(nTrain+nValid):]).sum()/nTest,2),\n",
    "      'bernoulli:', np.round(np.abs(ber_error[(nTrain+nValid):]).sum()/nTest,2),\n",
    "      'gamma:', np.round(np.abs(gam_error[(nTrain+nValid):]).sum()/nTest,2),\n",
    "      'SNG:', np.round(np.abs(sngr_error[(nTrain+nValid):]).sum()/nTest,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute CI coverage rate when varying multiple confidence levels (this can be slow)\n",
    "width_med_ci, width_mean_ci, conf_rate_ci = dec.getVaryCI(Y_const, hd_const, hd_bins_use, \n",
    "                                                          sngr_lik_mat, poi_lik_mat,\n",
    "                                                          ber_lik_mat, gam_lik_mat,\n",
    "                                                          poi_mean_decode,ber_mean_decode, \n",
    "                                                          sngr_mean_decode, gam_mean_decode,\n",
    "                                                          nTrain, nValid,nTest);\n",
    "# results are in order SNG, poisson, bernoulli, gamma\n",
    "# if only want to compute CI for SNG model, please refer to the getVaryCI function in decoding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot log posterior likelihood trace\n",
    "# resemble the data in time order\n",
    "xx = np.asarray(np.linspace(0, Y_real_enc.shape[0]-1, Y_real_enc.shape[0]), dtype='int')\n",
    "xx_split = np.array_split(xx, perm.shape[0]);\n",
    "\n",
    "rlt = [];\n",
    "trial_rlt = [];\n",
    "for ii in range(perm.shape[0]):\n",
    "    rlt = rlt + list(xx_split[perm[ii]]);\n",
    "    trial_rlt = trial_rlt + list(np.tile(ii,xx_split[perm[ii]].shape[0]))\n",
    "    \n",
    "order = np.argsort(rlt);\n",
    "trial_order = np.zeros(Y_real.shape[0])-1;\n",
    "trial_order[tempp] = np.array(trial_rlt)[order];\n",
    "\n",
    "poi_decode_all = np.zeros(Y_real.shape[0]);\n",
    "ber_decode_all = np.zeros(Y_real.shape[0]);\n",
    "sngr_decode_all = np.zeros(Y_real.shape[0]);\n",
    "gam_decode_all = np.zeros(Y_real.shape[0]);\n",
    "\n",
    "poi_decode_all[tempp] = poi_mean_decode[order];\n",
    "poi_decode_all[tempq] = poi_mean_decode2;\n",
    "\n",
    "sngr_decode_all[tempp] = sngr_mean_decode[order];\n",
    "sngr_decode_all[tempq] = sngr_mean_decode2;\n",
    "\n",
    "ber_decode_all[tempp] = ber_mean_decode[order];\n",
    "ber_decode_all[tempq] = ber_mean_decode2;\n",
    "\n",
    "gam_decode_all[tempp] = gam_mean_decode[order];\n",
    "gam_decode_all[tempq] = ber_mean_decode2;\n",
    "\n",
    "poi_lik_all = np.zeros((Y_real.shape[0], hd_bins_use.shape[0]));\n",
    "ber_lik_all = np.zeros((Y_real.shape[0], hd_bins_use.shape[0]));\n",
    "sngr_lik_all = np.zeros((Y_real.shape[0], hd_bins_use.shape[0]));\n",
    "gam_lik_all = np.zeros((Y_real.shape[0], hd_bins_use.shape[0]));\n",
    "\n",
    "poi_lik_all[tempp] = poi_lik_mat[order];\n",
    "poi_lik_all[tempq] = poi_lik_mat2;\n",
    "\n",
    "sngr_lik_all[tempp] = sngr_lik_mat[order];\n",
    "sngr_lik_all[tempq] = sngr_lik_mat2;\n",
    "\n",
    "ber_lik_all[tempp] = ber_lik_mat[order];\n",
    "ber_lik_all[tempq] = ber_lik_mat2;\n",
    "\n",
    "gam_lik_all[tempp] = gam_lik_mat[order];\n",
    "gam_lik_all[tempq] = gam_lik_mat2;\n",
    "\n",
    "hd_bin_num = uplot.getBinNum(hd, hd_bins_use);\n",
    "poi_bin_num = uplot.getBinNum(poi_decode_all, hd_bins_use);\n",
    "ber_bin_num = uplot.getBinNum(ber_decode_all, hd_bins_use);\n",
    "gam_bin_num = uplot.getBinNum(gam_decode_all, hd_bins_use);\n",
    "sngr_bin_num = uplot.getBinNum(sngr_decode_all, hd_bins_use);\n",
    "\n",
    "s_te = 4200;\n",
    "e_te = s_te+3000;\n",
    "\n",
    "poi_temp = np.clip(np.log(poi_lik_all/poi_lik_all.max(axis=1,keepdims=True)),-10,0);\n",
    "ber_temp = np.clip(np.log(ber_lik_all/ber_lik_all.max(axis=1,keepdims=True)),-10,0);\n",
    "gam_temp = np.clip(np.log(gam_lik_all/gam_lik_all.max(axis=1,keepdims=True)),-10,0);\n",
    "sngr_temp = np.clip(np.log(sngr_lik_all/sngr_lik_all.max(axis=1,keepdims=True)),-10,0);\n",
    "\n",
    "fig_conf = uplot.plotPosSuper2(hd_bin_num[s_te:e_te],sngr_temp[s_te:e_te].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot CI converage rate vs confidence levels and CI converage rate vs CI width\n",
    "conf_level_list=np.array(list(np.linspace(0.9,0.999,10))[:-1] + [1-1e-2, 1-5e-3, 1-1e-3]);\n",
    "print(conf_level_list);\n",
    "\n",
    "fig_cimr = uplot.plotCIcov_mean(conf_rate_ci, width_mean_ci,drop=True);\n",
    "fig_cilr = uplot.plotCIcov_lev(-np.log10(1-conf_rate_ci), -np.log10(1-conf_level_list),real=True,drop=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save results\n",
    "encoding_results = {'poi':[spl_values_poi,yfit_poi,cost_poi,test_cost_poi],'ber':[spl_values_ber, yfit_ber, cost_ber, test_cost_ber],\n",
    "                    'sngr':[spl_values_sngr, spl_p_values_sngr, spl_theta_values_sngr, sngr_k, sngr_loc, yfit_sngr, sngr_p, sngr_theta, cost_sngr, test_cost_sngr],\n",
    "                    'gam':[spl_values_gam,spl_k_values_gam,spl_theta_values_gam,yfit_gam,gam_k,gam_theta,cost_gam,test_cost_gam]};\n",
    "\n",
    "decoding_results = {'poi':[poi_mean_decode,poi_decode,poi_lik_mat],'ber':[ber_mean_decode,ber_decode,ber_lik_mat],\n",
    "                    'sngr':[sngr_mean_decode,sngr_decode,sngr_lik_mat],'gam':[gam_mean_decode,gam_decode,gam_lik_mat],\n",
    "                    'stats':[width_med_ci,width_mean_ci,conf_rate_ci]};\n",
    "\n",
    "np.savez(path+'rlts.npz',encoding_results,decoding_results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
